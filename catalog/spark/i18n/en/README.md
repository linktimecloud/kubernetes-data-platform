### 1. Introduce

Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.

### 2. Key features

#### Batch/streaming data

Unify the processing of your data in batches and real-time streaming, using your preferred language: Python, SQL, Scala, Java or R.

#### SQL analytics

Execute fast, distributed ANSI SQL queries for dashboarding and ad-hoc reporting. Runs faster than most data warehouses.

#### Data science at scale

Perform Exploratory Data Analysis (EDA) on petabyte-scale data without having to resort to downsampling

#### Machine learning

Train machine learning algorithms on a laptop and use the same code to scale to fault-tolerant clusters of thousands of machines.

### 3. Application scenarios

Apache Spark is a general-purpose distributed processing system for big data workloads. It has been deployed in every type of big data use case to detect patterns and provide real-time insights.

#### Financial Services

Spark is used in banking to predict customer churn and recommend new financial products. In investment banking, Spark is used to analyze stock prices and predict future trends.

#### medical insurance

Spark is being used to build the full spectrum of patient care, providing data to frontline healthcare workers at every patient interaction. Spark can also be used to predict/recommend treatment options for patients.

#### Manufacturing

Spark is being used to suggest when to perform preventive maintenance, thereby avoiding downtime of networked equipment.

#### retail

Spark attracts and retains customers through its personalized services and products.